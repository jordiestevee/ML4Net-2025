{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb666402",
   "metadata": {},
   "source": [
    "# Seminar 5 - Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d87ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c4139",
   "metadata": {},
   "source": [
    " ## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044dbc0",
   "metadata": {},
   "source": [
    "- We load the datasets for **10 clients**.\n",
    "- Each client has:\n",
    "    - `client_{i}_features.csv`: Wi-Fi CSI measurements (270 features per sample).\n",
    "    - `client_{i}_labels.csv`: Corresponding pose labels (integers from 1 to 12).\n",
    "- **Test Data**:\n",
    "  - A separate test set:\n",
    "    - `test_features.csv` and `test_labels.csv` containing **500 samples**.\n",
    "\n",
    "The CSI data represent Wi-Fi signal reflections when subjects perform different poses, collected for human pose estimation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c453e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 270)\n",
      "(64,)\n",
      "(500, 270)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dataset_Seminar5'\n",
    "num_clients = 10\n",
    "client_data = {}\n",
    "\n",
    "# Load each client’s training data\n",
    "for i in range(1, num_clients + 1):\n",
    "    X_path = os.path.join(data_dir, f'client_datasets/client_{i}_features.csv')\n",
    "    y_path = os.path.join(data_dir, f'client_datasets/client_{i}_labels.csv')\n",
    "    \n",
    "    X = pd.read_csv(X_path, header=None).values  # shape: (num_samples, 270)\n",
    "    y = pd.read_csv(y_path, header=None).values.flatten()  # shape: (num_samples,)\n",
    "    \n",
    "    client_data[i] = {'X': X, 'y': y}\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(os.path.join(data_dir, 'test_features.csv'), header=None).values\n",
    "y_test = pd.read_csv(os.path.join(data_dir, 'test_labels.csv'), header=None).values.flatten()\n",
    "\n",
    "print(X.shape)  # shape: (num_samples, 270)\n",
    "print(y.shape)  # shape: (num_samples,)\n",
    "print(X_test.shape)  # shape: (num_samples, 270)\n",
    "print(y_test.shape)  # shape: (num_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759472ee",
   "metadata": {},
   "source": [
    "We will adjust test labels from 0-11 instead of 1-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b388d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client_id in client_data:\n",
    "    client_data[client_id]['y'] -= 1  # Now ranges 0-11\n",
    "\n",
    "y_test -= 1  # Also adjust test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b0c94",
   "metadata": {},
   "source": [
    "We tried training with different models to see which one fits the most:\n",
    "### `PoseClassifierFC` (Fully Connected Network)\n",
    "- **Type**: Simple Feedforward Neural Network (MLP).\n",
    "- **Architecture**:\n",
    "  - Input: 270 features (flattened CSI data).\n",
    "  - Two hidden layers:\n",
    "    - `Linear(270 → 128)`, `ReLU`, `Dropout(0.3)`.\n",
    "    - `Linear(128 → 64)`, `ReLU`.\n",
    "  - Output layer: `Linear(64 → 12 classes)`.\n",
    "- **Use Case**: \n",
    "  - Lightweight.\n",
    "  - Good for quick testing or clients with very limited compute power.\n",
    "\n",
    "---\n",
    "\n",
    "### `ResSim` (Simplified Residual CNN)\n",
    "- **Type**: Lightweight ResNet-style Convolutional Neural Network.\n",
    "- **Architecture**:\n",
    "  - Input reshaped to (3 channels × 30 × 3 matrix).\n",
    "  - **Two residual blocks**:\n",
    "    - `Conv → ReLU → Conv` + Skip Connection → `ReLU → MaxPool`.\n",
    "  - Flatten the output.\n",
    "  - Fully Connected layer for classification.\n",
    "- **Use Case**:\n",
    "  - Leverages **residual connections** for better training stability.\n",
    "  - Suitable for capturing spatial patterns with limited depth.\n",
    "\n",
    "---\n",
    "\n",
    "### `PoseClassifierCNN` (Standard CNN)\n",
    "- **Type**: Regular Convolutional Neural Network.\n",
    "- **Architecture**:\n",
    "  - Input reshaped to (3 × 30 × 3).\n",
    "  - Two convolutional blocks:\n",
    "    - `Conv → ReLU → BatchNorm → MaxPool`.\n",
    "  - Flatten the output.\n",
    "  - Fully Connected layer → Dropout → Final classification layer.\n",
    "- **Use Case**:\n",
    "  - A classic CNN approach.\n",
    "  - Good balance between expressiveness and simplicity.\n",
    "  - Well-suited for spatial data like Wi-Fi CSI matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d875c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseClassifierFC(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(PoseClassifierFC, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(270, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efedb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResSim(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified ResNet-style network with residual connections and sequential blocks.\n",
    "    \n",
    "    Adapted for CSI input (flattened to 3x30x3).\n",
    "\n",
    "    Architecture:\n",
    "        - Two residual blocks: Conv → ReLU → Conv + skip\n",
    "        - Each followed by MaxPool\n",
    "        - Fully connected classifier\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=12): # 12 different pose classes\n",
    "        super(ResSim, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.shortcut1 = nn.Conv2d(3, 64, kernel_size=1)  # aligns input channels\n",
    "\n",
    "        # Block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 1)\n",
    "        self.fc = nn.Linear(64 * 7 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 30, 3)  # reshape input vector (270,) → (3, 30, 3)\n",
    "\n",
    "        # First residual connection\n",
    "        residual = self.shortcut1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.relu(x + residual)\n",
    "        x = self.pool(x) # (64, 15, 3)\n",
    "\n",
    "        # Second residual connection (no need for shortcut: same shape)\n",
    "        residual = x\n",
    "        x = self.block2(x)\n",
    "        x = self.relu(x + residual)\n",
    "        x = self.pool(x) # (64, 7, 3)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512118c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PoseClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(PoseClassifierCNN, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),  # (3, 30, 3) → (16, 30, 3)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # (16, 15, 3)\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # (32, 15, 3)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))  # (32, 7, 3)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                    # (32 * 7 * 3)\n",
    "            nn.Linear(32 * 7 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)     # output logits for 12 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input from (batch_size, 270) → (batch_size, 3, 30, 3)\n",
    "        x = x.view(-1, 3, 30, 3)  # match channel-first format\n",
    "        x = self.cnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7416b",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "`eval_model` Function:\n",
    "\n",
    "Evaluates a model’s accuracy:\n",
    "\n",
    "- **Set eval mode**: Disables layers like dropout.\n",
    "- **Convert** inputs to tensors.\n",
    "- **Predict** class labels (`argmax` over logits).\n",
    "- **Compute accuracy**: Percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X, y):\n",
    "    model.eval()  # Set model to evaluation mode (important: disables dropout, batchnorm updates)\n",
    "    \n",
    "    # Convert input features and labels to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)  # Features: float32\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)     # Labels: int64 (long)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation (saves memory and speeds up evaluation)\n",
    "        logits = model(X_tensor)     # Forward pass: compute raw scores (logits)\n",
    "        preds = torch.argmax(logits, dim=1)  # Get class with highest score (predicted label)\n",
    "        accuracy = ((preds == y_tensor).float().mean().item()) * 100  # Compute accuracy as percentage\n",
    "    \n",
    "    return accuracy  # Return accuracy %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38404eb",
   "metadata": {},
   "source": [
    "`eval_loss`\n",
    "- **Evaluates loss** for a model on given data.\n",
    "- **No gradient** computation (`torch.no_grad()`).\n",
    "- Uses the provided **loss criterion** (e.g., CrossEntropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(model, X, y, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)  # Features tensor\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)     # Labels tensor\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        logits = model(X_tensor)      # Forward pass\n",
    "        loss = criterion(logits, y_tensor)  # Compute loss\n",
    "    return loss.item()  # Return loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed6182",
   "metadata": {},
   "source": [
    "`weighted_train_loss`\n",
    "- **Purpose**: Compute the weighted average loss across selected clients.\n",
    "- For each client:\n",
    "  - Load global model weights.\n",
    "  - Evaluate local loss.\n",
    "- **Weight** each client's loss by its number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f971da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_train_loss(model_class, global_state, selected_clients, client_data, criterion):\n",
    "    train_losses = []\n",
    "    weights = []\n",
    "    \n",
    "    for k in selected_clients:\n",
    "        model = model_class(num_classes=12)          # Initialize model\n",
    "        model.load_state_dict(global_state)          # Load global model state\n",
    "        loss = eval_loss(model, client_data[k]['X'], client_data[k]['y'], criterion)  # Client loss\n",
    "        train_losses.append(loss)\n",
    "        weights.append(len(client_data[k]['X']))     # Weight: number of samples\n",
    "    \n",
    "    alpha = [w / sum(weights) for w in weights]  # Compute sample ratio per client\n",
    "    weighted_loss = sum(a * l for a, l in zip(alpha, train_losses))  # Weighted average loss\n",
    "    \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26c01c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1dc22",
   "metadata": {},
   "source": [
    "`initialize_global_model`\n",
    "- Creates a fresh model and saves its initial weights.\n",
    "\n",
    "`select_clients`\n",
    "- Randomly picks a subset of clients for the current round.\n",
    "\n",
    "`local_train`\n",
    "- Each selected client:\n",
    "  - Loads the global model.\n",
    "  - Trains locally using its private data.\n",
    "  - Returns updated weights and number of samples.\n",
    "\n",
    "`aggregate_models`\n",
    "- Aggregates client models using **FedAvg**:\n",
    "  - Weighted average of parameters based on client data sizes.\n",
    "\n",
    "`federated_learning`\n",
    "- Orchestrates the full FL process:\n",
    "  - For each round:\n",
    "    - Select clients.\n",
    "    - Perform local training.\n",
    "    - Aggregate updates.\n",
    "  - Every 5 rounds:\n",
    "    - Report test accuracy and weighted train loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d4d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "def initialize_global_model(model_class, num_classes=12):\n",
    "    \"\"\"Initialize the global model and save its initial state.\"\"\"\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    return model, deepcopy(model.state_dict())\n",
    "\n",
    "def select_clients(client_ids, num_clients_per_round):\n",
    "    \"\"\"Randomly select a subset of clients.\"\"\"\n",
    "    return random.sample(client_ids, num_clients_per_round)\n",
    "\n",
    "def local_train(model_class, global_state, client_dataset, local_epochs, batch_size, lr):\n",
    "    \"\"\"Train local model on client's data.\"\"\"\n",
    "    model = model_class(num_classes=12)\n",
    "    model.load_state_dict(global_state)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Prepare DataLoader\n",
    "    X_local = torch.tensor(client_dataset['X'], dtype=torch.float32)\n",
    "    y_local = torch.tensor(client_dataset['y'], dtype=torch.long)\n",
    "    dataset = torch.utils.data.TensorDataset(X_local, y_local)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Local training loop\n",
    "    for _ in range(local_epochs):\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return deepcopy(model.state_dict()), len(X_local)\n",
    "\n",
    "def aggregate_models(local_states, local_sizes):\n",
    "    \"\"\"Aggregate local models using FedAvg.\"\"\"\n",
    "    total_samples = sum(local_sizes)\n",
    "    new_global_state = deepcopy(local_states[0])\n",
    "    \n",
    "    for key in new_global_state:\n",
    "        new_global_state[key] = sum(\n",
    "            (local_states[i][key] * (local_sizes[i] / total_samples) for i in range(len(local_states)))\n",
    "        )\n",
    "    \n",
    "    return new_global_state\n",
    "\n",
    "def federated_learning(model_class, client_data, num_rounds, clients_per_round, local_epochs, batch_size, lr):\n",
    "    \"\"\"Main Federated Learning orchestration loop.\"\"\"\n",
    "    client_ids = list(client_data.keys())\n",
    "    global_model, global_state = initialize_global_model(model_class)\n",
    "    \n",
    "    for round_idx in range(num_rounds):\n",
    "        selected = select_clients(client_ids, clients_per_round)\n",
    "        local_states, local_sizes = [], []\n",
    "        \n",
    "        # Local training on selected clients\n",
    "        for k in selected:\n",
    "            state, size = local_train(model_class, global_state, client_data[k], local_epochs, batch_size, lr)\n",
    "            local_states.append(state)\n",
    "            local_sizes.append(size)\n",
    "        \n",
    "        # Update global model\n",
    "        global_state = aggregate_models(local_states, local_sizes)\n",
    "        global_model.load_state_dict(global_state)\n",
    "        \n",
    "        print(f\"Round {round_idx+1}/{num_rounds} complete.\")\n",
    "        \n",
    "        if (round_idx + 1) % 5 == 0:\n",
    "            train_accuracy = eval_model(global_model, X_test, y_test)\n",
    "            print(f\"Round {round_idx+1}: Test Accuracy = {train_accuracy:.4f} %\")\n",
    "            loss = weighted_train_loss(model_class, global_state, selected, client_data, nn.CrossEntropyLoss())\n",
    "            print(f\"Round {round_idx+1}: Weighted Train Loss = {loss:.4f}\")\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9d35b",
   "metadata": {},
   "source": [
    "Defining the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff396bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learning Parameters\n",
    "NUM_CLIENTS = 10\n",
    "CLIENTS_PER_ROUND = 5\n",
    "FL_ROUNDS = 30\n",
    "\n",
    "# Local training parameters\n",
    "LOCAL_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7d109",
   "metadata": {},
   "source": [
    "Initialize the model and train it. The evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/100 complete.\n",
      "Round 2/100 complete.\n",
      "Round 3/100 complete.\n",
      "Round 4/100 complete.\n",
      "Round 5/100 complete.\n",
      "Round 5: Test Accuracy = 41.8000 %\n",
      "Round 5: Weighted Train Loss = 0.7563\n",
      "Round 6/100 complete.\n",
      "Round 7/100 complete.\n",
      "Round 8/100 complete.\n",
      "Round 9/100 complete.\n",
      "Round 10/100 complete.\n",
      "Round 10: Test Accuracy = 39.8000 %\n",
      "Round 10: Weighted Train Loss = 1.0997\n",
      "Round 11/100 complete.\n",
      "Round 12/100 complete.\n",
      "Round 13/100 complete.\n",
      "Round 14/100 complete.\n",
      "Round 15/100 complete.\n",
      "Round 15: Test Accuracy = 46.8000 %\n",
      "Round 15: Weighted Train Loss = 0.6431\n",
      "Round 16/100 complete.\n",
      "Round 17/100 complete.\n",
      "Round 18/100 complete.\n",
      "Round 19/100 complete.\n",
      "Round 20/100 complete.\n",
      "Round 20: Test Accuracy = 49.0000 %\n",
      "Round 20: Weighted Train Loss = 0.5877\n",
      "Round 21/100 complete.\n",
      "Round 22/100 complete.\n",
      "Round 23/100 complete.\n",
      "Round 24/100 complete.\n",
      "Round 25/100 complete.\n",
      "Round 25: Test Accuracy = 52.8000 %\n",
      "Round 25: Weighted Train Loss = 0.5408\n",
      "Round 26/100 complete.\n",
      "Round 27/100 complete.\n",
      "Round 28/100 complete.\n",
      "Round 29/100 complete.\n",
      "Round 30/100 complete.\n",
      "Round 30: Test Accuracy = 54.2000 %\n",
      "Round 30: Weighted Train Loss = 0.5337\n",
      "Round 31/100 complete.\n",
      "Round 32/100 complete.\n",
      "Round 33/100 complete.\n",
      "Round 34/100 complete.\n",
      "Round 35/100 complete.\n",
      "Round 35: Test Accuracy = 55.8000 %\n",
      "Round 35: Weighted Train Loss = 0.2000\n",
      "Round 36/100 complete.\n",
      "Round 37/100 complete.\n",
      "Round 38/100 complete.\n",
      "Round 39/100 complete.\n",
      "Round 40/100 complete.\n",
      "Round 40: Test Accuracy = 54.8000 %\n",
      "Round 40: Weighted Train Loss = 0.3491\n",
      "Round 41/100 complete.\n",
      "Round 42/100 complete.\n",
      "Round 43/100 complete.\n",
      "Round 44/100 complete.\n",
      "Round 45/100 complete.\n",
      "Round 45: Test Accuracy = 55.2000 %\n",
      "Round 45: Weighted Train Loss = 0.1923\n",
      "Round 46/100 complete.\n",
      "Round 47/100 complete.\n",
      "Round 48/100 complete.\n",
      "Round 49/100 complete.\n",
      "Round 50/100 complete.\n",
      "Round 50: Test Accuracy = 55.2000 %\n",
      "Round 50: Weighted Train Loss = 0.4572\n",
      "Round 51/100 complete.\n",
      "Round 52/100 complete.\n",
      "Round 53/100 complete.\n",
      "Round 54/100 complete.\n",
      "Round 55/100 complete.\n",
      "Round 55: Test Accuracy = 56.2000 %\n",
      "Round 55: Weighted Train Loss = 0.1028\n",
      "Round 56/100 complete.\n",
      "Round 57/100 complete.\n",
      "Round 58/100 complete.\n",
      "Round 59/100 complete.\n",
      "Round 60/100 complete.\n",
      "Round 60: Test Accuracy = 55.2000 %\n",
      "Round 60: Weighted Train Loss = 0.4160\n",
      "Round 61/100 complete.\n",
      "Round 62/100 complete.\n",
      "Round 63/100 complete.\n",
      "Round 64/100 complete.\n",
      "Round 65/100 complete.\n",
      "Round 65: Test Accuracy = 59.0000 %\n",
      "Round 65: Weighted Train Loss = 0.3969\n",
      "Round 66/100 complete.\n",
      "Round 67/100 complete.\n",
      "Round 68/100 complete.\n",
      "Round 69/100 complete.\n",
      "Round 70/100 complete.\n",
      "Round 70: Test Accuracy = 56.4000 %\n",
      "Round 70: Weighted Train Loss = 0.3633\n",
      "Round 71/100 complete.\n",
      "Round 72/100 complete.\n",
      "Round 73/100 complete.\n",
      "Round 74/100 complete.\n",
      "Round 75/100 complete.\n",
      "Round 75: Test Accuracy = 54.8000 %\n",
      "Round 75: Weighted Train Loss = 0.4678\n",
      "Round 76/100 complete.\n",
      "Round 77/100 complete.\n",
      "Round 78/100 complete.\n",
      "Round 79/100 complete.\n",
      "Round 80/100 complete.\n",
      "Round 80: Test Accuracy = 58.2000 %\n",
      "Round 80: Weighted Train Loss = 0.4337\n",
      "Round 81/100 complete.\n",
      "Round 82/100 complete.\n",
      "Round 83/100 complete.\n",
      "Round 84/100 complete.\n",
      "Round 85/100 complete.\n",
      "Round 85: Test Accuracy = 58.6000 %\n",
      "Round 85: Weighted Train Loss = 0.3090\n",
      "Round 86/100 complete.\n",
      "Round 87/100 complete.\n",
      "Round 88/100 complete.\n",
      "Round 89/100 complete.\n",
      "Round 90/100 complete.\n",
      "Round 90: Test Accuracy = 56.8000 %\n",
      "Round 90: Weighted Train Loss = 0.4513\n",
      "Round 91/100 complete.\n",
      "Round 92/100 complete.\n",
      "Round 93/100 complete.\n",
      "Round 94/100 complete.\n",
      "Round 95/100 complete.\n",
      "Round 95: Test Accuracy = 57.8000 %\n",
      "Round 95: Weighted Train Loss = 0.4005\n",
      "Round 96/100 complete.\n",
      "Round 97/100 complete.\n",
      "Round 98/100 complete.\n",
      "Round 99/100 complete.\n",
      "Round 100/100 complete.\n",
      "Round 100: Test Accuracy = 56.6000 %\n",
      "Round 100: Weighted Train Loss = 0.1770\n",
      "Final Test Accuracy: 56.6000\n"
     ]
    }
   ],
   "source": [
    "# Train the model using Federated Learning\n",
    "global_model = federated_learning(\n",
    "    model_class=PoseClassifierCNN,\n",
    "    client_data=client_data,\n",
    "    num_rounds=FL_ROUNDS,\n",
    "    clients_per_round=CLIENTS_PER_ROUND,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Evaluate the trained global model on the test set\n",
    "accuracy = eval_model(global_model, X_test, y_test)\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a24a728",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ResSim.forward() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model using Federated Learning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m global_model = \u001b[43mfederated_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResSim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFL_ROUNDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclients_per_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCLIENTS_PER_ROUND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOCAL_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Evaluate the trained global model on the test set\u001b[39;00m\n\u001b[32m     13\u001b[39m accuracy = eval_model(global_model, X_test, y_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mfederated_learning\u001b[39m\u001b[34m(model_class, client_data, num_rounds, clients_per_round, local_epochs, batch_size, lr)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Main Federated Learning orchestration loop.\"\"\"\u001b[39;00m\n\u001b[32m     52\u001b[39m client_ids = \u001b[38;5;28mlist\u001b[39m(client_data.keys())\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m global_model, global_state = \u001b[43minitialize_global_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m round_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rounds):\n\u001b[32m     56\u001b[39m     selected = select_clients(client_ids, clients_per_round)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36minitialize_global_model\u001b[39m\u001b[34m(model_class, num_classes)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize_global_model\u001b[39m(model_class, num_classes=\u001b[32m12\u001b[39m):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initialize the global model and save its initial state.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, deepcopy(model.state_dict())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jordi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jordi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: ResSim.forward() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "# Train the model using Federated Learning\n",
    "global_model = federated_learning(\n",
    "    model_class=ResSim(),\n",
    "    client_data=client_data,\n",
    "    num_rounds=FL_ROUNDS,\n",
    "    clients_per_round=CLIENTS_PER_ROUND,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Evaluate the trained global model on the test set\n",
    "accuracy = eval_model(global_model, X_test, y_test)\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using Federated Learning\n",
    "global_model = federated_learning(\n",
    "    model_class=PoseClassifierFC,\n",
    "    client_data=client_data,\n",
    "    num_rounds=FL_ROUNDS,\n",
    "    clients_per_round=CLIENTS_PER_ROUND,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Evaluate the trained global model on the test set\n",
    "accuracy = eval_model(global_model, X_test, y_test)\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250710f",
   "metadata": {},
   "source": [
    "The model which reached the best value accuracy is the CNN architecture, that reached a test accuracy 0f 58% and then dropped down due to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbc7939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/30 complete.\n",
      "Round 2/30 complete.\n",
      "Round 3/30 complete.\n",
      "Round 4/30 complete.\n",
      "Round 5/30 complete.\n",
      "Round 6/30 complete.\n",
      "Round 7/30 complete.\n",
      "Round 8/30 complete.\n",
      "Round 9/30 complete.\n",
      "Round 10/30 complete.\n",
      "Round 11/30 complete.\n",
      "Round 12/30 complete.\n",
      "Round 13/30 complete.\n",
      "Round 14/30 complete.\n",
      "Round 15/30 complete.\n",
      "Round 16/30 complete.\n",
      "Round 17/30 complete.\n",
      "Round 18/30 complete.\n",
      "Round 19/30 complete.\n",
      "Round 20/30 complete.\n",
      "Round 21/30 complete.\n",
      "Round 22/30 complete.\n",
      "Round 23/30 complete.\n",
      "Round 24/30 complete.\n",
      "Round 25/30 complete.\n",
      "Round 26/30 complete.\n",
      "Round 27/30 complete.\n",
      "Round 28/30 complete.\n",
      "Round 29/30 complete.\n",
      "Round 30/30 complete.\n",
      "Round 31/30 complete.\n",
      "Round 32/30 complete.\n",
      "Round 33/30 complete.\n",
      "Round 34/30 complete.\n",
      "Round 35/30 complete.\n",
      "Round 36/30 complete.\n",
      "Round 37/30 complete.\n",
      "Round 38/30 complete.\n",
      "Round 39/30 complete.\n",
      "Round 40/30 complete.\n",
      "Round 41/30 complete.\n",
      "Round 42/30 complete.\n",
      "Round 43/30 complete.\n",
      "Round 44/30 complete.\n",
      "Round 45/30 complete.\n",
      "Round 46/30 complete.\n",
      "Round 47/30 complete.\n",
      "Round 48/30 complete.\n",
      "Round 49/30 complete.\n",
      "Round 50/30 complete.\n",
      "Round 51/30 complete.\n",
      "Round 52/30 complete.\n",
      "Round 53/30 complete.\n",
      "Round 54/30 complete.\n",
      "Round 55/30 complete.\n",
      "Round 56/30 complete.\n",
      "Round 57/30 complete.\n",
      "Round 58/30 complete.\n",
      "Round 59/30 complete.\n",
      "Round 60/30 complete.\n",
      "Round 61/30 complete.\n",
      "Round 62/30 complete.\n",
      "Round 63/30 complete.\n",
      "Round 64/30 complete.\n",
      "Round 65/30 complete.\n",
      "Round 66/30 complete.\n",
      "Round 67/30 complete.\n",
      "Round 68/30 complete.\n",
      "Round 69/30 complete.\n",
      "Round 70/30 complete.\n",
      "Round 71/30 complete.\n",
      "Round 72/30 complete.\n",
      "Round 73/30 complete.\n",
      "Round 74/30 complete.\n",
      "Round 75/30 complete.\n",
      "Round 76/30 complete.\n",
      "Round 77/30 complete.\n",
      "Round 78/30 complete.\n",
      "Round 79/30 complete.\n",
      "Round 80/30 complete.\n",
      "Round 81/30 complete.\n",
      "Round 82/30 complete.\n",
      "Round 83/30 complete.\n",
      "Round 84/30 complete.\n",
      "Round 85/30 complete.\n",
      "Round 86/30 complete.\n",
      "Round 87/30 complete.\n",
      "Round 88/30 complete.\n",
      "Round 89/30 complete.\n",
      "Round 90/30 complete.\n",
      "Round 91/30 complete.\n",
      "Round 92/30 complete.\n",
      "Round 93/30 complete.\n",
      "Round 94/30 complete.\n",
      "Round 95/30 complete.\n",
      "Round 96/30 complete.\n",
      "Round 97/30 complete.\n",
      "Round 98/30 complete.\n",
      "Round 99/30 complete.\n",
      "Round 100/30 complete.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# 1. Initialize a global ML model, ω(t = 0)\n",
    "global_model = PoseClassifierCNN(num_classes=12)\n",
    "global_model_state = deepcopy(global_model.state_dict())\n",
    "\n",
    "# Helper: get number of samples for each client\n",
    "client_num_samples = {k: len(client_data[k]['X']) for k in client_data}\n",
    "\n",
    "# Federated Learning Loop\n",
    "for round_idx in range(100):\n",
    "    # 2. Select a subset of clients S ⊆ K\n",
    "    selected_clients = random.sample(list(client_data.keys()), CLIENTS_PER_ROUND)\n",
    "    \n",
    "    local_states = []\n",
    "    local_sizes = []\n",
    "    \n",
    "    # 3. Send the global model to the clients, retrain locally\n",
    "    for k in selected_clients:\n",
    "        local_model = PoseClassifierCNN(num_classes=12)\n",
    "        local_model.load_state_dict(global_model_state)\n",
    "        local_model.train()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        X_local = torch.tensor(client_data[k]['X'], dtype=torch.float32)\n",
    "        y_local = torch.tensor(client_data[k]['y'], dtype=torch.long)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X_local, y_local)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        for epoch in range(LOCAL_EPOCHS):\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = local_model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # 4. Retrieve the individual models ωk(t)\n",
    "        local_states.append(deepcopy(local_model.state_dict()))\n",
    "        local_sizes.append(len(X_local))\n",
    "    \n",
    "    # 5. Aggregate the individual contributions (FedAvg)\n",
    "    total_samples = sum(local_sizes)\n",
    "    new_global_state = deepcopy(global_model_state)\n",
    "    for key in new_global_state:\n",
    "        new_global_state[key] = sum(\n",
    "            (local_states[i][key] * (local_sizes[i] / total_samples) for i in range(len(local_states)))\n",
    "        )\n",
    "    global_model_state = new_global_state\n",
    "    global_model.load_state_dict(global_model_state)\n",
    "    \n",
    "    print(f\"Round {round_idx+1}/{FL_ROUNDS} complete.\")\n",
    "\n",
    "# The trained global_model now contains the aggregated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce1593d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5860\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the global_model on the test set\n",
    "global_model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = global_model(X_test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    accuracy = (preds == y_test_tensor).float().mean().item()\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
