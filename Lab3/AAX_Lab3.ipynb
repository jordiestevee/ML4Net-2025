{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShbB8CxBMSLH"
      },
      "source": [
        "#ML4Net - Lab 3\n",
        "\n",
        "## Team members\n",
        "\n",
        "Name 1 (NIA 1) | Name 2 (NIA 2) | Name 3 (NIA 3)\n",
        "\n",
        "## Description\n",
        "\n",
        "In this lab, you will train a time series forecaster using Federated Learning (FL).\n",
        "\n",
        "The dataset you are going to use contains data from multiple APs. In particular, each AP's dataset contains the following features:\n",
        "\n",
        "* `datetime` is the timestamp (date) at which the measurement was taken\n",
        "* `Bytes` is the number of Bytes contributed at the corresponding datetime\n",
        "* `Active Connections` is the number of active connections (users connected to the AP) at the corresponding datetime\n",
        "* `Active Users` is the number of active users at the corresponding datetime\n",
        "* `AP ID` is the ID of the AP where the measurement was taken\n",
        "\n",
        "Original dataset: Chen, W., Lyu, F., Wu, F., Yang, P., & Ren, J. (2021). Flag: Flexible, accurate, and long-time user load prediction in large-scale WiFi system using deep RNN. IEEE Internet of Things Journal, 8(22), 16510-16521.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "*   Follow the steps from this Notebook and complete the proposed exercises.\n",
        "*   Deliver the completed Notebook by uploading it to your Github repository.\n",
        "*   **Submission deadline: 8 June 2025 (EoB).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqio72iAuxNJ"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfR6f_lHpklt"
      },
      "source": [
        "1. Connect your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl_xYKv2oYRb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxCHlkswbru"
      },
      "source": [
        "2. Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o6I9dNnpwbUB",
        "outputId": "4a8c18bb-92a0-449e-c5b6-ca92efc26a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-30 11:24:35--  https://github.com/fwilhelmi/fwilhelmi.github.io/blob/master/files/datasetLab3.pkl\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘datasetLab3.pkl’\n",
            "\n",
            "datasetLab3.pkl         [ <=>                ] 177.71K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-05-30 11:24:35 (3.45 MB/s) - ‘datasetLab3.pkl’ saved [181974]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/fwilhelmi/fwilhelmi.github.io/blob/master/files/datasetLab3.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ZtSumzuZCy"
      },
      "source": [
        "3. Define the main path of the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYfdaJF-qr5w"
      },
      "outputs": [],
      "source": [
        "mypath=\"drive/MyDrive/AAX/Lab3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISnLhobTwh7l"
      },
      "source": [
        "4. Extract the source code to the destination folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rgEgC4fZwiLf"
      },
      "outputs": [],
      "source": [
        "mkdir $mypath; cp datasetLab3.pkl $mypath; cd $mypath;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97kVtaWOutFD"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ-ATejy308z"
      },
      "source": [
        "Load the data from the pickle file (https://docs.python.org/3/library/pickle.html) that contains the dataset.\n",
        "\n",
        "**Important:** If you have issues with the dataset after automatically downloading and extracting it, download it and move it manually to the indicated folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn9zlRDcenHJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "print(os.getcwd())\n",
        "print(os.path.exists('datasetLab3.pkl'))\n",
        "\n",
        "# Open and load the pickle file\n",
        "try:\n",
        "    with open('datasetLab3.pkl', 'rb') as f:\n",
        "        loaded_data = pickle.load(f)\n",
        "    print(\"Data loaded successfully:\")\n",
        "    print(loaded_data)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'datasetLab3.pkl' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTAXUX5lN9KF"
      },
      "source": [
        "## EXERCISES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weqFFp6eShJg"
      },
      "source": [
        "### Exercise 1:\n",
        "\n",
        "Analyze the data from the different APs and discuss their properites (stationarity, trends, seasonality).\n",
        "\n",
        "Select 2 or 3 key APs showing different properties and generate:\n",
        "\n",
        "* Line plots (to plot the load vs the time)\n",
        "* Autocorrelation plots (to show the relationship between past and future samples of the load)\n",
        "* An Augmented Dickey-Fuller test (to study the stationarity of the AP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3tirZCOSkNC"
      },
      "outputs": [],
      "source": [
        "# (response to exercise 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3vAU830SmuR"
      },
      "source": [
        "### Exercise 2:\n",
        "\n",
        "Prepare the data to be presented as a time series to the ML model. For that, you will have to split features from samples by differentiating between an observation window (e.g., 10 samples) and a prediction window (e.g., 2 samples). Use a sliding window to iterate over all the samples.\n",
        "\n",
        "Example: For an array [0,1,2,3,4,5], using an observation window $T_o$ = 3 and a prediction window $T_p$ = 1 would lead to the following time series data:\n",
        "* $x_1$ = [0,1,2], $y_1$ = [3]\n",
        "* $x_2$ = [1,2,3], $x_2$ = [4]\n",
        "* $x_3$ = [2,3,4], $y_3$ = [5]\n",
        "\n",
        "(where x are the features and y the labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTjjIcWDS3yC"
      },
      "outputs": [],
      "source": [
        "# (response to exercise 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hgfWEriSpMa"
      },
      "source": [
        "### Exercise 3:\n",
        "\n",
        "Split the time series data into train, test, and validation, based on your criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqgnnqflZ8R3"
      },
      "outputs": [],
      "source": [
        "# (response to exercise 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGXDiQQEZ5vL"
      },
      "source": [
        "### Exercise 4:\n",
        "Define a model (e.g., GRU, LSTM, CNN, Transformer) able to receive the time series the data you generated in the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AhYwVhNfWGA"
      },
      "outputs": [],
      "source": [
        "# (response to exercise 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy_SulYKeEn4"
      },
      "source": [
        "### Exercise 5:\n",
        "\n",
        "Train your model following two approaches:\n",
        "1. **Centralized model training:** Mix data from multiple APs to train the model in a centralized fashion.\n",
        "2. **Federated model training:** Consider APs as independent clients who contribute to training a global model by submitting local model updates.\n",
        "\n",
        "Show the results by calculating the following metrics from the de-normalized data:\n",
        "* Mean squared error (MSE)\n",
        "* Mean absolute error (MAE)\n",
        "* Mean absolute percentage error (MAPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB4TAqiAeNXk"
      },
      "outputs": [],
      "source": [
        "# (response to exercise 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LafMStmKfW4-"
      },
      "source": [
        "### Exercise 6 (EXTRA):\n",
        "\n",
        "Reconsider your design to improve the accuracy of your model. For that, you can find a better approach to split the data (e.g., using larger observation windows) and include additional features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Eb3K84dgTb1"
      },
      "outputs": [],
      "source": [
        "# (response to exercise 6)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
